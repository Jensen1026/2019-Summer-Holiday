
# 机器学习基本概念

---

[toc]

---

## 1.机器学习

&emsp;&emsp;机器学习是实现人工智能的一种途径。其注重算法的设计，让计算机能够自动地从数据中“学习”规律，并利用规律对未知数据进行预测。因为学习算法涉及了大量的统计学理论，与统计推断联系尤为紧密，所以也被称为统计学习方法。

&emsp;&emsp;核心是“使用算法解析数据，从中学习，然后对世界上的某件事情做出决定或预测”。这意味着，与其显式地编写程序来执行某些任务，不如教计算机如何开发一个算法来完成任务。有三种主要类型的机器学习：监督学习、非监督学习和强化学习(半监督学习)。

&emsp;&emsp;加载数据、预处理、模型拟合、预测。

![](https://raw.githubusercontent.com/Jensen1026/Pictures/master/machine%20learning.jpg)

## 2. 监督学习

&emsp;&emsp;监督学习涉及一组标记数据。计算机可以使用特定的模式来识别每种标记类型的新样本。从给定的训练数据集中学习出一个函数（模型参数），当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求包括输入输出，也可以说是特征和目标。训练集中的目标是由人标注的。

&emsp;&emsp;监督学习是训练神经网络和决策树的常见技术。这两种技术高度依赖事先确定的分类系统给出的信息，对于神经网络，分类系统利用信息判断网络的错误，然后不断调整网络参数。对于决策树，分类系统用它来判断哪些属性提供了最多的信息。

>监督学习的两种主要类型是**分类和回归**。

&emsp;&emsp;在分类中，机器被训练成将一个组划分为特定的类。分类的一个简单例子是电子邮件帐户上的垃圾邮件过滤器。过滤器分析你以前标记为垃圾邮件的电子邮件，并将它们与新邮件进行比较。如果它们匹配一定的百分比，这些新邮件将被标记为垃圾邮件并发送到适当的文件夹。那些比较不相似的电子邮件被归类为正常邮件并发送到你的邮箱。

&emsp;&emsp;第二种监督学习是回归。在回归中，机器使用先前的(标记的)数据来预测未知。天气应用是回归的好例子，使用气象事件的历史数据(即平均气温、湿度和降水量)，你的手机天气应用程序可以查看当前天气，并在未来的时间内对天气进行预测。

## 3. 非监督学习

&emsp;&emsp;输入数据没有被标记，也没有确定的结果。样本数据类别未知，需要根据样本间的相似性对样本集进行分类（聚类，clustering）试图使类内差距最小化，类间差距最大化。通俗点将就是实际应用中，不少情况下无法预先知道样本的标签，也就是说没有训练样本对应的类别，因而只能从原先没有样本标签的样本集开始学习分类器设计。

&emsp;&emsp;非监督学习目标不是告诉计算机怎么做，而是让它自己去学习怎样做事情。非监督学习有两种思路。第一种思路是在指导agent时不为其指定明确分类，而是在成功时，采用某种形式的激励制度。需要注意的是，这类训练通常会置于决策问题的框架里，因为它的目标不是为了产生一个分类系统，而是做出最大回报的决定，这种思路很好的概括了现实世界，agent可以对正确的行为做出激励，而对错误行为做出惩罚。

>非监督学习分为两大类

&emsp;&emsp;一类为基于概率密度函数估计的直接方法：指设法找到各类别在特征空间的分布参数，再进行分类。

&emsp;&emsp;另一类是称为基于样本间相似性度量的简洁聚类方法：其原理是设法定出不同类别的核心或初始内核，然后依据样本与核心之间的相似性度量将样本聚集成不同的类别。

>监督学习和非监督学习之间的不同点

1. 监督学习训练数据既有特征又有标签。通过训练让机器找到特征和标签之间的联系，在面对只有特征没有标签的数据时，可以判断出标签。
2. 非监督学习：我们不知道数据集中数据和特征之间的关系，而是要根据聚类或一定的模型得到数据之间的关系。比起监督学习，非监督学习更像是自学，让机器学会自己做事，数据是没有标签的。
3. 监督学习必须有训练集和测试样本。在训练集中找规律，而对测试样本使用这种规律。而非监督学习没有训练集，只有一组数据，在该组数据集内寻找规律。
4. 有监督学习的方法就是识别事物，识别的结果表现在给待识别数据加上了标签。因此训练样本集必须由带标签的样本组成。而非监督学习方法只有要分析的数据集的本身，预先没有什么标签。如果发现数据集呈现某种聚集性，则可按自然的聚集性分类，但不予以某种预先分类标签对上号为目的。
5. 非监督学习方法在寻找数据集中的规律性，这种规律性并不一定要达到划分数据集的目的，也就是说不一定要“分类”。(这一点上非监督学习比监督学习的用途要广)

## 4. 半监督学习

&emsp;&emsp;半监督学习介于监督学习和非监督学习之间，它同时利用有标记样本与无标记样本进行学习。它所利用的数据集可以分为两部分，一部分是有标记的数据集，另一部分是无标记数据集，这部分数据集中样本点的类别标记未知。 与实际情况相符一般假设无标记数据远远多于有标记数据。

>半监督学习中的基本假设(聚类假设、流型假设)

&emsp;&emsp;目前的机器学习技术大多基于独立同分布假设，即数据样本独立地采样于同一分布。除了独立同分布假设，为了学习到泛化的结果，监督学习技术大多基于平滑smoothness)假设，即相似或相邻的样本点的标记也应当相似。而在半监督学习中这种平滑假设则体现为两个较为常见的假设：聚类(cluster)假设与流型(manifold)假设。

&emsp;&emsp;聚类假设：是指同一聚类中的样本点很可能具有同样的类别标记。这个假设可以通过另一种等价的方式进行表达，那就是决策边界所穿过的区域应当是数据点较为稀疏的区域，因为如果决策边界穿过数据点较为密集的区域那就很有可能将一个聚类中的样本点分为不同的类别这与聚类假设矛盾。

&emsp;&emsp;流型假设：是指高维中的数据存在着低维的特性。

## 5. 深度学习

[深度学习学习笔记整理](https://www.cnblogs.com/mfryf/p/5946883.html)

## 6. 强化学习

## 7. 深度强化学习

## 8. Q学习

## 9. 迁移学习

## 10. 迭代学习

&emsp;&emsp;迭代是为了实现某种结果而重复一组任务的过程。大多数书都按照正向顺序（sequential）讲解机器学习的过程：加载数据、预处理、拟合模型、预测等。这种顺序方法当然是合理和有帮助的，但现实的机器学习很少如此线性。相反，实用机器学习有一个特殊的循环（cyclical）性质，需要不断的迭代、调整和改进。

## 11. 全连接神经网络

## 12. 卷积神经网络

1. 组成：输入层、激活函数、全连接层(输入层、输出层、隐藏层)
   ![](https://raw.githubusercontent.com/Jensen1026/Pictures/master/%E8%BE%93%E5%85%A5%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E8%BE%93%E5%87%BA%E5%B1%82.jpg)
2. 同一层的神经元之间没有连接；每个连接都有一个权值
   ![](https://raw.githubusercontent.com/Jensen1026/Pictures/master/%E5%B1%82%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB.jpg)
3. 一个神经元的组成为:
   * 输入：n维向量x
   * 线性加权：w是权值，b是偏置项
     ![](https://raw.githubusercontent.com/Jensen1026/Pictures/master/%E5%B1%82%E5%8A%A0%E6%9D%83%E5%85%AC%E5%BC%8F.jpg)
   * 激活函数：H(x)，要求非线性，容易求导数
   * 输出：a
4. 神经网络的训练
   * 一个神经网络的每个连接上的权值
   * 神经网络就是一个模型，这些权值就是模型的参数(即模型要学习的东西)
   * 对于这个神经网络的连接方式、网络层数、每层的节点个数，这些是我们实现设置的，成为超参数。


## 13. 深度残差网络

## 14. 马尔可夫决策过程

## 15. 随机梯度下降算法(梯度下降法)

## 16. 策略梯度的深度强化学习

## 17. 经验回放机制

## 18. 注意力机制

## 19. 反馈控制

## 20. A3C算法
