
# 机器学习基本概念

---

[toc]

---

## 1.机器学习

&emsp;&emsp;机器学习是实现人工智能的一种途径。其注重算法的设计，让计算机能够自动地从数据中“学习”规律，并利用规律对未知数据进行预测。因为学习算法涉及了大量的统计学理论，与统计推断联系尤为紧密，所以也被称为统计学习方法。

&emsp;&emsp;核心是“使用算法解析数据，从中学习，然后对世界上的某件事情做出决定或预测”。这意味着，与其显式地编写程序来执行某些任务，不如教计算机如何开发一个算法来完成任务。有三种主要类型的机器学习：监督学习、非监督学习和强化学习(半监督学习)。

![](https://raw.githubusercontent.com/Jensen1026/Pictures/master/machine%20learning.jpg)

## 2. 监督学习

&emsp;&emsp;监督学习涉及一组标记数据。计算机可以使用特定的模式来识别每种标记类型的新样本。从给定的训练数据集中学习出一个函数（模型参数），当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求包括输入输出，也可以说是特征和目标。训练集中的目标是由人标注的。

&emsp;&emsp;监督学习是训练神经网络和决策树的常见技术。这两种技术高度依赖事先确定的分类系统给出的信息，对于神经网络，分类系统利用信息判断网络的错误，然后不断调整网络参数。对于决策树，分类系统用它来判断哪些属性提供了最多的信息。

>监督学习的两种主要类型是**分类和回归**。

&emsp;&emsp;在分类中，机器被训练成将一个组划分为特定的类。分类的一个简单例子是电子邮件帐户上的垃圾邮件过滤器。过滤器分析你以前标记为垃圾邮件的电子邮件，并将它们与新邮件进行比较。如果它们匹配一定的百分比，这些新邮件将被标记为垃圾邮件并发送到适当的文件夹。那些比较不相似的电子邮件被归类为正常邮件并发送到你的邮箱。

&emsp;&emsp;第二种监督学习是回归。在回归中，机器使用先前的(标记的)数据来预测未知。天气应用是回归的好例子，使用气象事件的历史数据(即平均气温、湿度和降水量)，你的手机天气应用程序可以查看当前天气，并在未来的时间内对天气进行预测。

## 3. 非监督学习

&emsp;&emsp;输入数据没有被标记，也没有确定的结果。样本数据类别未知，需要根据样本间的相似性对样本集进行分类（聚类，clustering）试图使类内差距最小化，类间差距最大化。通俗点将就是实际应用中，不少情况下无法预先知道样本的标签，也就是说没有训练样本对应的类别，因而只能从原先没有样本标签的样本集开始学习分类器设计。

&emsp;&emsp;非监督学习目标不是告诉计算机怎么做，而是让它自己去学习怎样做事情。非监督学习有两种思路。第一种思路是在指导agent时不为其指定明确分类，而是在成功时，采用某种形式的激励制度。需要注意的是，这类训练通常会置于决策问题的框架里，因为它的目标不是为了产生一个分类系统，而是做出最大回报的决定，这种思路很好的概括了现实世界，agent可以对正确的行为做出激励，而对错误行为做出惩罚。

>非监督学习分为两大类

&emsp;&emsp;一类为基于概率密度函数估计的直接方法：指设法找到各类别在特征空间的分布参数，再进行分类。

&emsp;&emsp;另一类是称为基于样本间相似性度量的简洁聚类方法：其原理是设法定出不同类别的核心或初始内核，然后依据样本与核心之间的相似性度量将样本聚集成不同的类别。

>监督学习和非监督学习之间的不同点

1. 监督学习训练数据既有特征又有标签。通过训练让机器找到特征和标签之间的联系，在面对只有特征没有标签的数据时，可以判断出标签。
2. 非监督学习：我们不知道数据集中数据和特征之间的关系，而是要根据聚类或一定的模型得到数据之间的关系。比起监督学习，非监督学习更像是自学，让机器学会自己做事，数据是没有标签的。
3. 监督学习必须有训练集和测试样本。在训练集中找规律，而对测试样本使用这种规律。而非监督学习没有训练集，只有一组数据，在该组数据集内寻找规律。
4. 有监督学习的方法就是识别事物，识别的结果表现在给待识别数据加上了标签。因此训练样本集必须由带标签的样本组成。而非监督学习方法只有要分析的数据集的本身，预先没有什么标签。如果发现数据集呈现某种聚集性，则可按自然的聚集性分类，但不予以某种预先分类标签对上号为目的。
5. 非监督学习方法在寻找数据集中的规律性，这种规律性并不一定要达到划分数据集的目的，也就是说不一定要“分类”。(这一点上非监督学习比监督学习的用途要广)

## 4. 半监督学习

&emsp;&emsp;半监督学习介于监督学习和非监督学习之间，它同时利用有标记样本与无标记样本进行学习。它所利用的数据集可以分为两部分，一部分是有标记的数据集，另一部分是无标记数据集，这部分数据集中样本点的类别标记未知。 与实际情况相符一般假设无标记数据远远多于有标记数据。

>半监督学习中的基本假设(聚类假设、流型假设)

&emsp;&emsp;目前的机器学习技术大多基于独立同分布假设，即数据样本独立地采样于同一分布。除了独立同分布假设，为了学习到泛化的结果，监督学习技术大多基于平滑smoothness)假设，即相似或相邻的样本点的标记也应当相似。而在半监督学习中这种平滑假设则体现为两个较为常见的假设：聚类(cluster)假设与流型(manifold)假设。

&emsp;&emsp;聚类假设：是指同一聚类中的样本点很可能具有同样的类别标记。这个假设可以通过另一种等价的方式进行表达，那就是决策边界所穿过的区域应当是数据点较为稀疏的区域，因为如果决策边界穿过数据点较为密集的区域那就很有可能将一个聚类中的样本点分为不同的类别这与聚类假设矛盾。

&emsp;&emsp;流型假设：是指高维中的数据存在着低维的特性。

## 5. 深度学习

[深度学习学习笔记整理](https://www.cnblogs.com/mfryf/p/5946883.html)

## 6. 强化学习

## 7. 深度强化学习

## 8. Q学习

## 9. 迁移学习

## 10. 迭代学习

&emsp;&emsp;迭代是为了实现某种结果而重复一组任务的过程。大多数书都按照正向顺序（sequential）讲解机器学习的过程：加载数据、预处理、拟合模型、预测等。这种顺序方法当然是合理和有帮助的，但现实的机器学习很少如此线性。相反，实用机器学习有一个特殊的循环（cyclical）性质，需要不断的迭代、调整和改进。

## 11. 全连接神经网络

>全连接神经网络：对n-1层和n层而言，n-1层的任意一个节点，都和第n层所有节点有连接。即第n层的每个节点在进行计算的时候，激活函数的输入是n-1层所有节点的加权。

>“全连接”是一种不错的连接方式，但是当网络很大时，训练速度很慢。部分连接解释认为的切断两个节点直接的连接，这样训练时计算量将会大大减小。

1. 组成：输入层、激活函数、全连接层(输入层、输出层、隐藏层)
   ![](https://raw.githubusercontent.com/Jensen1026/Pictures/master/%E8%BE%93%E5%85%A5%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E8%BE%93%E5%87%BA%E5%B1%82.jpg)
2. 同一层的神经元之间没有连接；每个连接都有一个权值
   ![](https://raw.githubusercontent.com/Jensen1026/Pictures/master/%E5%B1%82%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB.jpg)
3. 一个神经元的组成为:
   * 输入：n维向量x
   * 线性加权：w是权值，b是偏置项
     ![](https://raw.githubusercontent.com/Jensen1026/Pictures/master/%E5%B1%82%E5%8A%A0%E6%9D%83%E5%85%AC%E5%BC%8F.jpg)
   * 激活函数：H(x)，要求非线性，容易求导数
   * 输出：a
4. 神经网络的训练
   * 一个神经网络的每个连接上的权值；
   * 神经网络就是一个模型，这些权值就是模型的参数(即模型要学习的东西)；
   * 对于这个神经网络的连接方式、网络层数、每层的节点个数，这些是我们实现设置的，成为超参数。
5. 激活函数
   * 人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端
   * 激活函数将非线性特征引入到我们的网络中。在神经元中，输入的inputs通过加权求和后，还被作用了一个函数，这个函数就是激活函数。引入激活函数就是为了增加神经网络模型的非线性。没有激活函数的每层相当于矩阵相乘，即使叠加了若干层后，本质上还是矩阵的乘法。
   * 常见的激活函数
     * Sigmoid函数：f(x)=1/(1+e^(-x))；1).两头过于平坦(梯度更新十分缓慢，即梯度消失)；2).输出值域不对称(不是以0为均值)；3).该函数可以用在网络最后一层，作为输出层进行二分类，尽量不用在隐藏层
     * Tanh函数：f(x)=(e^x-e^(-x))/(e^x+e^(-x))；1).两头过于平坦(梯度更新十分缓慢，即梯度消失)
     * ReLU函数：f(x)=max(0,x)；1).当输入为整数时，不存在梯度小时的问题；2).当输入为负数时，会产生梯度消失问题；3).计算速度要快很多。
   * 如何选择激活函数：
     * 深度学习往往需要大量时间来处理数据，模型的收敛速度尤为重要。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。
     * 如果使用ReLU，那么一定要小心设置learning rate，而且要注意不要让网络出现很多"dead"神经元，如果这个问题不好解决，那么可以试试Leaky ReLU、PReLU或者Maxout。
     * 最好不要用sigmoid，你可以试试tanh，不过可以预期它的效果会比不上ReLU和Maxout。
   * Leaky ReLU函数
   * MaxOut函数
6. 损失函数

   >损失函数分为**经验风险损失函数**和**结构风险损失函数**，前者反映的是预测结果和实际结果之间的差别，后者则是经验风险损失函数加上正则项。

   * 损失函数是将随机事件或其有关随机变量的取值映射为非负数以表示该随机时间的风险或者损失的函数。在应用中，损失函数通常作为学习准则与优化问题相联系，即通过最小化损失函数求解和评估模型。

## 12. 卷积神经网络

>在全连接神经网络中，每两层之间的所有节点都有边相连。卷积神经网络也是通过一层一层的节点组织起来的，对于卷积神经网络，相邻两层之间只有部分节点相连。在卷积神经网络的前几层中，每一层的节点都被组织成一个三维矩阵。前几层中每一个节点只和上一层中部分节点相连。

卷积神经网络是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。它包括卷积层(convolutional layer)和池化层(pooling layer)。  
卷积神经网络组成：输入层、卷积层、激活函数、池化层、全连接层

## 13. 深度残差网络

## 14. 马尔可夫决策过程

## 15. 随机梯度下降算法(梯度下降法)

## 16. 策略梯度的深度强化学习

## 17. 经验回放机制

## 18. 注意力机制

## 19. 反馈控制

## 20. A3C算法
