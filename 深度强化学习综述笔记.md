
# 深度强化学习综述笔记

---

[toc]

---

## 一、引言

>&emsp;&emsp;**深度学习**(DL)的基本思想是通过多层的网络结构和非线性变换，组合低层特征，形成抽象的、易于区分的高层表示，以发现数据的分布式特征表示。因此**DL方法侧重于对事物的感知和表达**。

>&emsp;&emsp;**强化学习**(RL)的基本思想是通过最大化智能体从环境中获得的累计奖赏值，以学习到完成目标的策略。因此**RL方法更加侧重于学习解决问题的策略**。

&emsp;&emsp;在越来越多复杂的现实场景任务中，需要利用DL来自动学习大规模输入数据的抽象表征，并以此表征为依据进行自我激励的RL，优化解决问题的策略。

&emsp;&emsp;深度强化学习创新性地将具有感知能力的DL和具有决策能力的RL相结合。agent对自身知识的构建和学习都直接来自原始输入信号，无需任何的人工编码和领域知识，因此**DRL是一种端对端的感知与控制系统，具有很强的通用性**。其学习过程可以描述为：

1. 在每个时刻agent与环境交互得到一个高维度的观察，并利用DL方法来感知观察，以得到具体的状态特征表示；

2. 基于预期回报来评价各动作的价值函数，并通过某种策略将当前状态映射为相应的动作；

3. 环境对此动作做出反应，并得到下一个观察。通过不断循环以上过程，最终可以得到实现目标的最优策略。

## 二、预备知识

### 2.1 深度学习

>&emsp;&emsp;DL的概念源于人工神经网络，含多隐藏层的多层感知器是DL模型的一个典型范例。DL模型通常由多层的非线性运算单元组合而成。其将较低层的输出作为更高一层的输入，通过这种方式自动地从大量训练数据中学习抽象的特征表示，以发现数据的分布式特征。

>&emsp;&emsp;训练深层神经网络的基本原则：先用**非监督学习**对网络逐层进行贪婪的预训练，再用**监督学习**对整个网络进行微调。这种预训练的方式为深度神经网络提供了较理想的初始参数，降低了深度神经网络的优化难度。

随着训练数据的增长和计算能力的提升，卷积神经网络开始被广泛应用。卷积神经网络主要四个发展方向：

1. 增加网络的层数

2. 增加卷积模块的功能

3. 增加网络层数和卷积模块功能

4. 增加新的网络模块，向卷积神经网络中加入循环神经网络、注意力机制等结构

### 2.2 强化学习

>&emsp;&emsp;RL是一种从环境状态映射到动作的学习，目标是使agent在与环境的交互过程中获得最大的累积奖赏。马尔可夫决策过程可以用来对RL问题进行建模。

## 三、基于值函数的深度强化学习

### 3.1 深度Q网络

&emsp;&emsp;Mnith等人**将卷积神经网络与传统RL中的Q学习算法相结合，提出了深度Q网络(Deep Q-Network,DQN)模型，该模型用于处理基于视觉感知的控制任务，是DRL领域的开创性工作**。

**模型结构：**

&emsp;&emsp;DQN模型的输入是距离当前时刻最近的4幅预处理后的图像。该输入经过3个卷积层和2个**全连接层**的非线性变换，最终在输出层产生每个动作的Q值。

**训练算法：**

为缓解非线性网络表示值函数时出现的不稳定等问题，**DQN主要对传统的Q学习算法做了三处改进**：

1. **DQN在训练过程中使用经验回放机制**，**在线处理得到的转移样本，在每个时间步t，将agent与环境交互得到的转移样本存储到回放记忆单元中。训练时，每次从D中随机抽取小批量的转移样本，并使用随机梯度下降算法更新网络参数θ**。在训练深度网络时，通常要求样本之间是相互独立的。这种随机采样的方式，大大降低了样本之间的关联性，从而提升了算法的稳定性。

2. DQN除了使用深度卷积网络近似表示当前的值函数之外，还**单独使用了另一个网络来产生目标Q值**。引入目标值网络后，在一段时间内目标Ｑ值是保持不变的，一定程度上降低了当前Ｑ值和目标Ｑ值之间的相关性，提升了算法的稳定性。

3. **DQN将奖赏值和误差项缩小到有限的区间内**，保证了Q值和梯度值都处于合理的范围内，提高了算法的稳定性。在解决各类基于视觉感知的DRL任务时，DQN使用了同一套网络模型、参数设置和训练算法，这充分说明了DQN方法具有很强的适应性和通用性。

### 3.2 深度Q网络训练算法的改进

**深度双Q网络：**

&emsp;&emsp;在DQN中使用近似表示值函数的优化目标时，每次都选取下一个状态中最大Ｑ值所对应的动作．选择和评价动作都是基于目标值网络的参数θ-，这会引起在学习过程中出现过高估计Q值的问题。

>&emsp;&emsp;**van Hasselt**等人于双Ｑ学习算法提出了深度双Ｑ网络算法。在双Ｑ学习中有两套不同的参数：θ和θ-。其中θ用来选择对应最大Q值的动作，θ-用来评估最优动作的Ｑ值。两套参数将动作选择和策略评估分离开，降低了过高估计Ｑ值的风险。因此DDQN使用当前值网络的参数θ来选择最优动作，使用目标值网络的参数θ-来评估该最优动作。

**基于优势学习的深度Q网络：**

>&emsp;&emsp;由上可知，**降低Q值的评估误差可以提升性能**。Bellemare等人在贝尔曼方程中定义新的操作符，来增大最优动作值和次优动作值之间的差异，以减小每次都选取下一状态中最大Q值对应动作所带来的评估误差。

&emsp;&emsp;**分别定义优势学习(AL)误差项和和一致性优势学习(PAL)误差项。实验表明用AL和PAL误差项来替代贝尔曼方程中的误差项可以有效的增加最优和次优动作之间的差异，从而获得更加精确的Q值**。即在DQN中加入AL和PAL误差项可以有效的减小评估Q值的偏差，促进学习效果的进一步提升。其中采用PAL误差项时，最优和次优动作对应致函数之间的差异更大，Q值的估计也更加精确。

**基于优先采样的深度Q网络：**

>&emsp;&emsp;DQN为了消除转移样本之间的相关性，使用经验回放机制在线地存储和使用agent与环境交互得到的历史样本。**在每个时刻，经验回放机制从样本池中等概率地抽取小批量的样本用于训练．然而等概率采样并不能区分不同样本的重要性，同时由于样本池D的存储量有限，某些样本还未被充分利用就已经被舍弃**。(缺点、不足)

>&emsp;&emsp;针对该问题，**Schaul**等人在DDQN的基础上提出了一种基于比例优先级采样的深度双Ｑ网络。**该方法用基于优先级的采样方式来替代均匀采样，提高一些有价值样本的采样概率**，从而加快最优策略的学习。

&emsp;&emsp;该抽样方法将每个样本的时间差分误差项作为评价优先级的标准。该误差为r+γmaxa'Ｑ(s',a'|θ-)-Q(s,a|θ)，并且其绝对值越大，对应样本被采样的概率越高。

&emsp;&emsp;在抽样过程中该方法使用随机比例化和重要性采样权重两种技巧。其中，**随机比例化**操作不仅能充分利用较大TD误差项(时间差分误差项)对应的样本，而且保证了抽取样本的多样性。**重要性采样权重**的使用放缓了参数更新的速度，保证了学习的稳定性。实验表明，基于该抽样方式的深度双Ｑ网络可以提升训练速度，并在很多Atari 2600游戏中获得了更高的分数。

### 3.3 DQN模型结构的改进

>&emsp;&emsp;对DQN模型的改进一般是通过向原有网络中添加新的功能模块来实现的。例如，可以向DQN模型中加入循环神经网络结构，使得模型拥有时间轴上的记忆能力。本节主要介绍两种重要的DQN模型的改进版本，分别是基于竞争架构的DQN和深度循环Ｑ网络。

**基于竞争架构的DQN：**

&emsp;&emsp;在很多基于视觉感知的DRL任务中，受不同动作的影响，状态动作对的值函数是不同的。然而在某些状态下，值函数的大小是与动作无关的。

>&emsp;&emsp;利用上述思想，Wang等人设计了一种竞争网络结构，并将其加入到DQN网络模型中。该网络结构与DQN模型的不同之处在于：DQN将CNN提取的抽象特征经过全连接层后，直接在输出层输出对应动作的Q值，而引入竞争网络结构的模型则将CNN提取的抽象特征分流 到两个支路中，其中一路代表**状态值函数(状态值流)**，另一路代表依赖状态的**动作优势函数(动作优势流)**。通过该种竞争网络结构，agent可以在策略评估过程中更快地识别出正确的行为。

**深度循环Q网络：**

&emsp;&emsp;在传统的RL方法中，状态信息的部分可观察性一直是个亟待解决的难题。DQN通过堆叠离当前时刻最近的４幅历史图像组成输入状态，虽然有效缓解了状态信息的部分可观察问题，却增加了网络的计算和存储负担。

&emsp;&emsp;针对此问题，Hausknecht等人利用循环神经网络结构来记忆时间轴上连的历史状态信息，提出了DRQN模型。

&emsp;&emsp;DRQN将DQN中第1个全连接1层的部件替换成了256个长短期记忆单元。此时模型的输入仅为当前时刻的一幅图像，减少了深度网络感知图像特征所耗费的计算资源。实验表明，在部分状态可观察的情况下，DRQN表现出比DQN更好的性能。因此DRQN模型适用于普遍存在部分状态可观察问题的复杂任务。

&emsp;&emsp;随着DL领域中各种新颖网络模块的提出，未来DRL模型会朝着结构多样化、模块复杂化的方向发展。例如，**可以利用深度残差网络所具备的强大感知能力来提高agent对复杂状态空间的表征效果**；另外，**可以在模型中加入视觉注意力机制(Visual Attention Mechanism,VAM)使得agent在不同状态下将注意力集中到有利于做出决策的区域**，从而加速学习的进程。

## 四、基于策略梯度的深度强化学习

>&emsp;&emsp;策略梯度是一种常用的策略优化方法，它通过**不断计算策略期望总奖赏关于策略参数的梯度来更新策略参数，最终收敛于最优策略**。因此在解决DRL问题时，可以采用参数为θ的深度神经网络来进行参数化表示策略，并利用策略梯度方法来优化策略。

>&emsp;&emsp;值得注意的是，**在求解DRL问题时，往往第一选择是采取基于策略梯度的算法。原因是它能够直接优化策略的期望总奖赏，并以端对端的方式直接在策略空间中搜索最优策略，省去了繁琐的中间环节**。因此与DQN及其改进模型相比，基于策略梯度的DRL方法使用范围更广，策略优化的效果更好。

### 4.1 深度策略梯度的起源和发展

>&emsp;&emsp;策略梯度方法是一种直接使用逼近器来近似表示和优化策略，最终得到最优策略的方法。策略梯度最常见的思想是增加总奖赏较高情节出现的概率。

>&emsp;&emsp;在大规模状态的DRL任务中，可以通过深度神经网络参数化表示策略，并采用传统的策略梯度方法来求解最优策略。

>&emsp;&emsp;优化策略的另一种思路是增加“好”的动作出现的概率。在RL中一般是通过优势函数评价动作的好坏，因此可以利用优势函数项来构造策略梯度。

>&emsp;&emsp;此外，**深度策略梯度方法的另一个研究方向是通过增加额外的人工监督来促进策略搜索**。例如著名的AlphaGo围棋机器人，先使用监督学习从人类专家的棋局中预测人类的走子行为，再用策略梯度方法针对赢得围棋比赛的真实目标进行精细的策略参数调整。**然而在某些任务是缺乏监督数据的，比如现实场景下的机器人控制，可以通过引导式策略搜索方法来监督策略搜索的过程**。在只接受原始输入信号的真实场景中，引导式策略搜索实现了对机器人的操控。

### 4.2 基于行动者评论家的深度策略梯度方法

&emsp;&emsp;上述深度策略梯度方法的基本思想是通过各种策略梯度方法直接优化用深度神经网络参数化表示的策略。这类方法在每个迭代步，都需要采样批量大小为N的轨迹来更新策略梯度。

&emsp;&emsp;然而在许多复杂的现实场景中，很难**在线**获得大量训练数据。例如在真实场景下机器人的操控任务中，在线收集并利用大量训练数据会产生十分昂贵的代价，且**动作连续的特性使得在线抽取批量轨迹的方式无法达到令人满意的覆盖面。以上问题会导致局部最优解的出现**。针对此问题，可以将传统RL中的行动者评论家(Actor-Critic,AC)框架拓展到深度策略梯度方法中.

&emsp;&emsp;**基于AC框架的深度确定性策略梯度(Deep deterministic Policy Gradient,DDPG)算法，该算法可以解决连续动作空间上的DRL问题**。

&emsp;&emsp;实验表明，**DDPG不仅在一系列连续动作空间的任务中表现稳定**，而且求得最优解所需要的时间步也远远少于DQN。与基于值函数的DRL方法相比，基于AC框架的深度策略梯度方法(DDPG)优化策略效率更高、求解速度更快。

&emsp;&emsp;然而**在有噪声干扰的复杂环境下**，策略一般都具有一定的随机性。**DDPG使用确定性的策略梯度方法**。对于随机环境的场景，该方法并不适用。针对此问题，Heess等人提出了**一种适用于连续动作空间任务的通用框架，称为随机值梯度(Stochastic Value Gradient,SVG)方法**。SVG使用再参数化的数学技巧来学习环境动态性的生成模型，将确定性策略梯度方法扩展为一种随机环境下的策略优化过程。

### 4.3 异步的优势行动者评论家算法

&emsp;&emsp;不同类型的深度神经网络为DRL中策略优化任务提供了高效运行的表征形式。为了缓解传统策略梯度方法与神经网络结合时出现的不稳定性，各类深度策略梯度方法（如DDPG,SVG等）都采用了经验回放机制来消除训练数据间的相关性。**然而经验回放机制存在两个不足之处：(1)agent与环境的每次实时交互都需要耗费很多的内存和计算力；(2)经验回放机制要求agent采用离策略（off-policy）方法来进行学习，而离策略方法只能基于旧策略生成的数据进行更新**。

>&emsp;&emsp;Mnith等人根据异步强化学习(Asynchronous Reinforcement Learning,ARL)的思想，提出了一种轻量级的DRL算法。其中**异步的优势行动这评论家算法(Asynchronous Advantage Actor-Critic,A3C)**在各类连续动作框架的控制任务上表现最好。

>&emsp;&emsp;具体地，A3C算法利用CPU多线程的功能并行、异步地执行多个agent。因此在任意时刻，并行的agent都将会经历许多不同的状态，去除了训练过程中产生的状态转移样本之间的关联性。因此这种低消耗的异步执行方式可以很好地替代经验回放机制。

>&emsp;&emsp;A3C算法在训练时降低了对硬件的要求。深度策略梯度算法十分依赖计算能力很强的图形处理器(GPU)，而A3C算法在实际的操作过程中只需要一个标准的多核CPU。

>&emsp;&emsp;A3C算法能够广泛应用于各种2D、3D离散和连续动作空间的任务，并且在这些任务中都取得了最佳的效果。这说明A3C是目前最通用和最成功的一种DRL算法。当然，将A3C与近期的一些深度策略梯度算法相结合可能会进一步提升其性能。

## 五、基于搜索与监督的深度强化学习

&emsp;&emsp;核心思想：可以通过增加额外的人工监督来促进策略搜索的过程。**蒙特卡洛树搜索(Montc Carlo Tree Search,MCTS)作为一种经典的启发式策略搜索方法**，被广泛用于游戏博弈问题中的行动规划。因此在基于搜索与监督的DRL方法中，策略搜索一般是通过MCTS来完成的。

### 5.1 结合神经网络和MCTS

&emsp;&emsp;难点在于：存在的状态空间巨大且精确评估棋盘布局、走子困难。AlphaGo的主要思想有两点：(1)使用MCTS来近似估计每个状态的值函数；(2)使用基于值函数的CNN来评估棋盘的当前布局和走子。

>AlphGo完整的学习系统主要由以下四个部分组成：
(1)**策略网络**。又分为监督学习的策略网络和RL的策略网络。策略网络的作用是根据当前的布局来预测和采样下一步走棋。
(2)**滚轮策略**。目标也是预测下一步走子，但是预测的速度是策略网络的1000倍。
(3)**估值网络**。根据当前局面，估计双方获胜的概率。
(4)**MCTS**。将策略网络、滚轮策略和估值网络融合进策略搜索的过程中，以形成一个完整的系统。

## 六、深度强化学习的应用

>&emsp;&emsp;在DRL发展的最初阶段，DQN算法主要被应用于Atari 2600平台中的各类2D视频游戏中。随后，研究人员分别从算法和模型两方面对DQN进行了改进，使得agent在Atari 2600游戏中的平均得分提高了300%，并在模型中加入记忆和推理模块，成功地将DRL应用场景拓宽到3D场景下的复杂任务中。AlphaGo围棋算法结合深度神经网络和MCTS，成功地击败了围棋世界冠军。

1. 深度强化学习在机器人控制领域的应用
2. 深度强化学习在计算机视觉领域的应用
3. 深度强化学习在自然语言处理领域的应用
4. 深度强化学习在参数优化中的应用
5. 深度强化学习在博弈领域的应用
