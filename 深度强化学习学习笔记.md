
# 深度强化学习学习笔记

1. 预备知识
   * DL
   * RL
   * DRL的起步
2. 三类主要的深度强化学习方法
   * 基于值函数的深度强化学习
   * 基于策略梯度的深度强化学习
   * 基于搜索和监督的深度强化学习
3. 深度强化学习的前沿研究方向
   * 分层深度强化学习
   * 多任务迁移深度强化学习
   * 多智能体深度强化学习
   * 基于记忆和推理的深度强化学习
   * DRL中探索和利用的平衡问题

## 一、引言

深度学习(DL)的基本思想是通过多层的网络结构和非线性变换，组合低层特征，形成抽象的、易于区分的高层表示，以发现数据的分布式特征表示。因此DL方法侧重于对事物的感知和表达。

强化学习(RL)的基本思想是通过最大化智能体从环境中获得的类集奖赏值，以学习到完成目标的策略。因此RL方法更加侧重于学习解决问题的策略。

在越来越多复杂的现实场景任务中，需要利用DL来自动学习大规模输入数据的抽象表征，并以此表征为依据进行自我激励的RL，优化解决问题的策略。

深度强化学习创新性地将具有感知能力的DL和具有决策能力的RL相结合。agent对自身知识的构建和学习都直接来自原始输入信号，无需任何的人工编码和领域知识，因此RL是一种端对端的感知与控制系统，具有很强的通用性。其学习过程可以描述为：(1)在每个时刻agent与环境交互得到一个高维度的观察，并利用DL方法来感知观察，以得到具体的状态特征表示；(2)基于预期回报来评价各动作的价值函数，并通过某种策略将当前状态映射为相应的动作；(3)环境对此动作做出反应，并得到下一个观察。通过不断循环以上过程，最终可以得到实现目标的最优策略。

## 二、预备知识

### 2.1 深度学习

DL的概念源于人工神经网络，含多隐藏层的多层感知器是DL模型的一个典型范例。DL模型通常由多层的非线性运算单元组合而成。其将较低层的输出作为更高一层的输入，通过这种方式自动地从大量训练数据中学习抽象的特征表示，以发现数据的分布式特征。

训练深层神经网络的基本原则：先用**非监督学习**对网络逐层进行贪婪的预训练，再用**监督学习**对整个网络进行微调。这种预训练的方式为深度神经网络提供了较理想的初始参数，降低了深度神经网络的优化难度。

随着训练数据的增长和计算能力的提升，卷积神经网络开始被广泛应用。卷积神经网络主要四个发展方向：
    1. 增加网络的层数
    2. 增加卷积模块的功能
    3. 增加网络层数和卷积模块功能
    4. 增加新的网络模块，向卷积神经网络中加入循环神经网络、注意力机制等结构

### 2.2 强化学习

ＲＬ是一种从环境状态映射到动作的学习，目标是使agent在与环境的交互过程中获得最大的累积奖赏。马尔可夫决策过程可以用来对RL问题进行建模。

### 2.3 深度强化学习的起步

## 三、基于值函数的深度强化学习

### 3.1 深度Q网络

Mnith等人将卷积神经网络与传统RL中的Q学习算法相结合，提出了深度Q网络(Deep Q-Network,DQN)模型，该模型用于处理基于视觉感知的控制任务，是DRL领域的开创性工作。

**模型结构**

DQN模型的输入是距离当前时刻最近的4幅预处理后的图像。该输入经过3个卷积层和2个全连接层的非线性变换，最终在输出层产生每个动作的Q值。

**训练算法**

为缓解非线性网络表示值函数时出现的不稳定等问题，DQN主要对传统的Q学习算法做了三处改进：
    1.DQN在训练过程中使用经验回放机制，在线处理得到的转移样本，在每个时间步 t，将agent与环境交互得到的转移样本存储到回放记忆单元中。训练时，每次从D中随机抽取小批量的转移样本，并使用随机梯度下降算法更新网络参数θ。在训练深度网络时，通常要求样本之间是相互独立的。这种随机采样的方式，大大降低了样本之间的关联性，从而提升了算法的稳定性。
    2.DQN除了使用深度卷积网络近似表示当前的值函数之外，还单独使用了另一个网络来产生目标Q值。引入目标值网络后，在一段时间内目标Ｑ值是保持不变的，一定程度上降低了当前Ｑ值和目标Ｑ值之间的相关性，提升了算法的稳定性。
    3.DQN将奖赏值和误差项缩小到有限的区间内，保证了Q值和梯度值都处于合理的范围内，提高了算法的稳定性。在解决各类基于视觉感知的DRL任务时，DQN使用了同一套网络模型、参数设置和训练算法，这充分说明了DQN方法具有很强的适应性和通用性。

### 3.2 深度Q网络训练算法的改进

**深度双Q网络**

在ＤＱＮ中使用近似表示值函数的优化目标时，每次都选取下一个状态中最大Ｑ值所对应的动作．选择和评价动作都是基于目标值网络的参数θ－，这会引起在学习过程中出现过高估计Q值的问题。
van Hasselt等人于双Ｑ学习算法提出了深度双Ｑ网络算法。在双Ｑ学习中有两套不同的参数：θ和θ－。其中θ用来选择对应最大Q值的动作，θ－用来评估最优动作的Ｑ值。两套参数将动作选择和策略评估分离开，降低了过高估计Ｑ值的风险。因此DDQN使用当前值网络的参数θ来选择最优动作，使用目标值网络的参数θ－来评估该最优动作。

**基于优势学习的深度Q网络**


